{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from tika import parser\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Users/Twishaa/OneDrive/Desktop/python/AI_NeuralNetworks/document classification/data/train_data\"\n",
    "\n",
    "# Change the directory\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read a text file.\n",
    "def read_text_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        parsed_pdf = parser.from_file(file_path)\n",
    "        \n",
    "        pdf_content = parsed_pdf['content']\n",
    "        return pdf_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_corpus(path):\n",
    "    \n",
    "    corpus = []\n",
    "    for file in os.listdir():\n",
    "        # Checking whether file is in text format or not\n",
    "        if file.endswith(\".pdf\"):\n",
    "            \n",
    "            file_path = f\"{path}\\\\{file}\"\n",
    "            pdf_content = read_text_file(file_path).lower()\n",
    "            ps = PorterStemmer()\n",
    "            wordnet = WordNetLemmatizer()\n",
    "            sentences = nltk.sent_tokenize(pdf_content)\n",
    "            for i in range(len(sentences)):\n",
    "                pdf_content_1 = re.sub(r'\\s+',' ', sentences[i])\n",
    "                pdf_content_1 = re.sub(r'\\W', '', pdf_content_1, flags=re.I)\n",
    "                pdf_content_1 = re.sub(r'\\s+', ' ', pdf_content_1, flags=re.I)\n",
    "                pdf_content_1 = pdf_content_1.split()\n",
    "                pdf_content_1 = [wordnet.lemmatize(word) for word in pdf_content_1 if not word in set(stopwords.words('english'))]\n",
    "                pdf_content_1 = ' '.join(pdf_content_1)\n",
    "                corpus.append(pdf_content_1)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cleaning the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ps = PorterStemmer()\n",
    "# wordnet = WordNetLemmatizer()\n",
    "# sentences = nltk.sent_tokenize(get_corpus(path))\n",
    "# for i in range (len(sentences)):\n",
    "#     # pdf_content_1 = [pdf_content.lower() for invoice in invoices]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-21 16:56:13,866 [MainThread  ] [INFO ]  Retrieving http://search.maven.org/remotecontent?filepath=org/apache/tika/tika-server/1.24/tika-server-1.24.jar to C:\\Users\\Twishaa\\AppData\\Local\\Temp\\tika-server.jar.\n",
      "2021-11-21 17:04:10,170 [MainThread  ] [INFO ]  Retrieving http://search.maven.org/remotecontent?filepath=org/apache/tika/tika-server/1.24/tika-server-1.24.jar.md5 to C:\\Users\\Twishaa\\AppData\\Local\\Temp\\tika-server.jar.md5.\n",
      "2021-11-21 17:04:12,061 [MainThread  ] [WARNI]  Failed to see startup log message; retrying...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_corpus = get_corpus(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b> One Class SVM</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 909)\t1.0\n",
      "  (1, 291)\t1.0\n",
      "  (2, 362)\t1.0\n",
      "  (3, 654)\t1.0\n",
      "  (4, 913)\t1.0\n",
      "  (5, 284)\t1.0\n",
      "  (6, 371)\t1.0\n",
      "  (7, 234)\t1.0\n",
      "  (8, 699)\t1.0\n",
      "  (9, 684)\t1.0\n",
      "  (10, 100)\t1.0\n",
      "  (11, 807)\t1.0\n",
      "  (12, 864)\t1.0\n",
      "  (13, 810)\t1.0\n",
      "  (14, 857)\t1.0\n",
      "  (15, 530)\t1.0\n",
      "  (16, 308)\t1.0\n",
      "  (17, 647)\t1.0\n",
      "  (18, 204)\t1.0\n",
      "  (19, 350)\t1.0\n",
      "  (20, 292)\t1.0\n",
      "  (21, 628)\t1.0\n",
      "  (22, 894)\t1.0\n",
      "  (23, 42)\t1.0\n",
      "  (24, 674)\t1.0\n",
      "  :\t:\n",
      "  (976, 363)\t1.0\n",
      "  (977, 766)\t1.0\n",
      "  (978, 413)\t1.0\n",
      "  (979, 584)\t1.0\n",
      "  (980, 773)\t1.0\n",
      "  (981, 297)\t1.0\n",
      "  (982, 761)\t1.0\n",
      "  (983, 303)\t1.0\n",
      "  (984, 790)\t1.0\n",
      "  (985, 365)\t1.0\n",
      "  (986, 766)\t1.0\n",
      "  (987, 416)\t1.0\n",
      "  (988, 587)\t1.0\n",
      "  (989, 582)\t1.0\n",
      "  (990, 298)\t1.0\n",
      "  (991, 759)\t1.0\n",
      "  (992, 304)\t1.0\n",
      "  (993, 162)\t1.0\n",
      "  (994, 422)\t1.0\n",
      "  (995, 861)\t1.0\n",
      "  (996, 421)\t1.0\n",
      "  (997, 400)\t1.0\n",
      "  (998, 594)\t1.0\n",
      "  (999, 442)\t1.0\n",
      "  (1000, 331)\t1.0\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(train_corpus)\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One Class SVM Model\n",
    "clf = OneClassSVM(nu=0.2, kernel=\"rbf\", gamma=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneClassSVM(gamma=5, nu=0.2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "clf.fit(X_train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = \"C:/Users/Twishaa/OneDrive/Desktop/python/AI_NeuralNetworks/document classification/data/test_data\"\n",
    "# test_corpus = get_corpus(test_path)\n",
    "# test_path = \"C:/Users/Twishaa/OneDrive/Desktop/python/AI_NeuralNetworks/document classification/data/test_data\"\n",
    "os.chdir(test_path)\n",
    "test_corpus = get_corpus(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (3, 473)\t1.0\n",
      "  (4, 691)\t1.0\n",
      "  (5, 696)\t1.0\n",
      "  (6, 150)\t1.0\n",
      "  (7, 146)\t1.0\n",
      "  (8, 142)\t1.0\n",
      "  (9, 662)\t1.0\n",
      "  (71, 611)\t1.0\n",
      "  (466, 611)\t1.0\n",
      "  (476, 611)\t1.0\n",
      "  (482, 193)\t1.0\n",
      "  (523, 611)\t1.0\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "X_test = vectorizer.transform(test_corpus)\n",
    "print(X_test)\n",
    "pred = clf.predict(X_test)\n",
    "# print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190720_018.pdf: 1\n",
      "AmazonWebServices.pdf: 1\n",
      "baubamaja_12305124.pdf: 1\n",
      "burgerking_20191019_016.pdf: 1\n",
      "cafecrepe_20180908_008.pdf: 1\n",
      "caffeine_1935658.pdf: 1\n",
      "caltrain-425345423423.pdf: 1\n",
      "data-06-00078-v2.pdf: 1\n",
      "flamingwok_20180908_003.pdf: 1\n",
      "free_fiber.pdf: 1\n",
      "hesburger_645907.pdf: 1\n",
      "hm_20180908_017.pdf: 1\n",
      "oldspaghettifactory_20180908_006.pdf: 1\n",
      "ozo-5056563.pdf: 1\n",
      "redrobin_20180908_013.pdf: 1\n",
      "Resume_1.pdf: 1\n",
      "safeway-3208524524.pdf: 1\n",
      "safeway_20180908_011.pdf: 1\n",
      "seatosky_20180908_005.pdf: 1\n",
      "seatosky_20180908_009.pdf: 1\n",
      "shakeshack_20181208_004.pdf: 1\n",
      "sliders-454353423425.pdf: 1\n",
      "swissotel_377904.pdf: 1\n",
      "sw_20181208_005.pdf: 1\n",
      "sw_20181208_3630.pdf: 1\n",
      "taxi_010E33SD547.pdf: 1\n",
      "translink_20180908_009.pdf: 1\n",
      "translink_20180908_012.pdf: 1\n",
      "translink_20180908_016.pdf: 1\n",
      "venetian_434280912998.pdf: 1\n",
      "walgreens-42543523432.pdf: 1\n",
      "wholefoods_20180908_010.pdf: 1\n",
      "wholefoods_20180908_014.pdf: 1\n"
     ]
    }
   ],
   "source": [
    "# print('Accuracy: ', accuracy_score(test_path, pred))\n",
    "for i, file in enumerate(os.listdir()):\n",
    "    print(f'{file}: {pred[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e1371b620b748fe5ef07373dbb171613f145a48f5b2ce9f7854211be71adf268"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
